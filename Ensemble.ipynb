{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71482383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___healthy', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_mosaic_virus', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n",
      "Total number of images found: 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = r\"D:\\ML\\Assignment 1\\tomato\\train\"\n",
    "\n",
    "# Get class names\n",
    "classes = os.listdir(dataset_path)\n",
    "print(f\"Classes found: {classes}\")\n",
    "\n",
    "# Dictionary to store image counts\n",
    "image_counts = {}\n",
    "all_images = []\n",
    "all_labels = []\n",
    "color_hist_features = []\n",
    "hog_features = []\n",
    "lbp_features = []\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Preprocess the image - resize, grayscale.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    return img_resized, img_gray\n",
    "\n",
    "def extract_color_histogram(img, bins=(16, 16, 16)):\n",
    "    \"\"\"Extract color histogram from the image.\"\"\"\n",
    "    hist = cv2.calcHist([img], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_hog_features(img_gray):\n",
    "    \"\"\"Extract HOG features from grayscale image.\"\"\"\n",
    "    features = hog(img_gray, pixels_per_cell=(8, 8), cells_per_block=(3, 3), \n",
    "                   visualize=False, block_norm='L2-Hys')\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_lbp_features(img_gray, radius=3, n_points=24):\n",
    "    \"\"\"Extract Local Binary Pattern (LBP) features from grayscale image.\"\"\"\n",
    "    lbp = local_binary_pattern(img_gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "# Read images and apply preprocessing\n",
    "total_images = 0\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    images = os.listdir(class_path)\n",
    "    image_counts[cls] = len(images)\n",
    "    total_images += len(images)\n",
    "    \n",
    "    for img_name in images[:100]:  # Increased sample size per class\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img_resized, img_gray = preprocess_image(img_path)\n",
    "        \n",
    "        all_images.append(img_resized)\n",
    "        all_labels.append(cls)\n",
    "        \n",
    "        color_hist_features.append(extract_color_histogram(img_resized))\n",
    "        hog_features.append(extract_hog_features(img_gray))\n",
    "        lbp_features.append(extract_lbp_features(img_gray))\n",
    "\n",
    "print(f\"Total number of images found: {total_images}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8834a667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___healthy', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_mosaic_virus', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n",
      "Total number of images found: 10000\n",
      "Label distribution: Counter({0: 100, 1: 100, 9: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 8: 100, 7: 100})\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Ensemble Model Accuracy: 94.50%\n",
      "Ensemble Classification Report:\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                      Tomato___Bacterial_spot       0.91      1.00      0.95        20\n",
      "                        Tomato___Early_blight       0.95      0.90      0.92        20\n",
      "                         Tomato___Late_blight       1.00      0.95      0.97        20\n",
      "                           Tomato___Leaf_Mold       1.00      0.95      0.97        20\n",
      "                  Tomato___Septoria_leaf_spot       0.95      0.90      0.92        20\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       0.83      1.00      0.91        20\n",
      "                         Tomato___Target_Spot       0.95      0.90      0.92        20\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.95      0.95      0.95        20\n",
      "                 Tomato___Tomato_mosaic_virus       1.00      0.95      0.97        20\n",
      "                             Tomato___healthy       0.95      0.95      0.95        20\n",
      "\n",
      "                                     accuracy                           0.94       200\n",
      "                                    macro avg       0.95      0.94      0.95       200\n",
      "                                 weighted avg       0.95      0.94      0.95       200\n",
      "\n",
      "âœ… Ensemble Model Training and Evaluation Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = r\"D:\\ML\\Assignment 1\\tomato\\train\"\n",
    "\n",
    "# Get class names\n",
    "classes = os.listdir(dataset_path)\n",
    "print(f\"Classes found: {classes}\")\n",
    "\n",
    "# Feature containers\n",
    "image_counts = {}\n",
    "all_images = []\n",
    "all_labels = []\n",
    "color_hist_features = []\n",
    "hog_features = []\n",
    "lbp_features = []\n",
    "\n",
    "# Functions for preprocessing and feature extraction\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    return img_resized, img_gray\n",
    "\n",
    "def extract_color_histogram(img, bins=(16, 16, 16)):\n",
    "    hist = cv2.calcHist([img], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_hog_features(img_gray):\n",
    "    return hog(img_gray, pixels_per_cell=(8, 8), cells_per_block=(3, 3), visualize=False, block_norm='L2-Hys')\n",
    "\n",
    "def extract_lbp_features(img_gray, radius=3, n_points=24):\n",
    "    lbp = local_binary_pattern(img_gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "# Load images and extract features\n",
    "total_images = 0\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    images = os.listdir(class_path)\n",
    "    image_counts[cls] = len(images)\n",
    "    total_images += len(images)\n",
    "    \n",
    "    for img_name in images[:100]:  # Limit per class\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img_resized, img_gray = preprocess_image(img_path)\n",
    "\n",
    "        all_images.append(img_resized)\n",
    "        all_labels.append(cls)\n",
    "\n",
    "        color_hist_features.append(extract_color_histogram(img_resized))\n",
    "        hog_features.append(extract_hog_features(img_gray))\n",
    "        lbp_features.append(extract_lbp_features(img_gray))\n",
    "\n",
    "print(f\"Total number of images found: {total_images}\")\n",
    "\n",
    "# Combine features\n",
    "X = np.hstack((hog_features, color_hist_features, lbp_features))\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LDA(n_components=None)\n",
    "X_lda = lda.fit_transform(X, Y)\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"Label distribution: {Counter(Y)}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lda, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "### === SVM ===\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_model = SVC()\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svm.fit(X_train, Y_train)\n",
    "best_svm_model = SVC(**grid_search_svm.best_params_)\n",
    "best_svm_model.fit(X_train, Y_train)\n",
    "\n",
    "### === Decision Tree ===\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(X_train, Y_train)\n",
    "best_dt_model = DecisionTreeClassifier(**grid_search_dt.best_params_, random_state=42)\n",
    "best_dt_model.fit(X_train, Y_train)\n",
    "\n",
    "### === Random Forest ===\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train, Y_train)\n",
    "best_rf_model = RandomForestClassifier(**grid_search_rf.best_params_, random_state=42)\n",
    "best_rf_model.fit(X_train, Y_train)\n",
    "\n",
    "### === Ensemble Model ===\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', best_svm_model),\n",
    "        ('dt', best_dt_model),\n",
    "        ('rf', best_rf_model)\n",
    "    ],\n",
    "    voting='hard'  # change to 'soft' if all models support predict_proba\n",
    ")\n",
    "\n",
    "ensemble_model.fit(X_train, Y_train)\n",
    "Y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(Y_test, Y_pred_ensemble)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_ensemble, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"âœ… Ensemble Model Training and Evaluation Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71cfe129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Label Encoder saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the ensemble model\n",
    "joblib.dump(ensemble_model, 'ensemble_model.pkl')\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "print(\"Model and Label Encoder saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdab39f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_svm_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load your trained models (make sure these are trained on THIS machine)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m best_svm_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_svm_model.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m best_dt \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_dt_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m best_rf_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_rf_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_svm_model.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your trained models (make sure these are trained on THIS machine)\n",
    "best_svm_model = joblib.load(\"best_svm_model.pkl\")\n",
    "best_dt = joblib.load(\"best_dt_model.pkl\")\n",
    "best_rf_model = joblib.load(\"best_rf_model.pkl\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('svm', best_svm_model),\n",
    "    ('dt', best_dt),\n",
    "    ('rf', best_rf_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Train ensemble model again if needed (skip if already trained)\n",
    "# ensemble_model.fit(X_train, Y_train)\n",
    "\n",
    "# Save models with your local environment\n",
    "joblib.dump(ensemble_model, \"ensemble_model.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bc883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___healthy', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_mosaic_virus', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Ensemble Accuracy: 99.95%\n",
      "Classification Report:\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                      Tomato___Bacterial_spot       1.00      1.00      1.00       200\n",
      "                        Tomato___Early_blight       1.00      1.00      1.00       200\n",
      "                         Tomato___Late_blight       1.00      1.00      1.00       200\n",
      "                           Tomato___Leaf_Mold       1.00      1.00      1.00       200\n",
      "                  Tomato___Septoria_leaf_spot       1.00      1.00      1.00       200\n",
      "Tomato___Spider_mites Two-spotted_spider_mite       1.00      1.00      1.00       200\n",
      "                         Tomato___Target_Spot       1.00      1.00      1.00       200\n",
      "       Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00       200\n",
      "                 Tomato___Tomato_mosaic_virus       1.00      0.99      1.00       200\n",
      "                             Tomato___healthy       1.00      1.00      1.00       200\n",
      "\n",
      "                                     accuracy                           1.00      2000\n",
      "                                    macro avg       1.00      1.00      1.00      2000\n",
      "                                 weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "âœ… All models and components saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = r\"D:\\ML\\Assignment 1\\tomato\\train\"\n",
    "\n",
    "# Get class names\n",
    "classes = os.listdir(dataset_path)\n",
    "print(f\"Classes found: {classes}\")\n",
    "\n",
    "# Storage\n",
    "all_images = []\n",
    "all_labels = []\n",
    "color_hist_features = []\n",
    "hog_features = []\n",
    "lbp_features = []\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_resized = cv2.resize(img, (128, 128))\n",
    "    img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    return img_resized, img_gray\n",
    "\n",
    "def extract_color_histogram(img, bins=(16, 16, 16)):\n",
    "    hist = cv2.calcHist([img], [0, 1, 2], None, bins, [0, 256]*3)\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_hog_features(img_gray):\n",
    "    features = hog(img_gray, pixels_per_cell=(8, 8), cells_per_block=(3, 3), \n",
    "                   visualize=False, block_norm='L2-Hys')\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_lbp_features(img_gray, radius=3, n_points=24):\n",
    "    lbp = local_binary_pattern(img_gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "# Read and process images\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    images = os.listdir(class_path)[:1000]  # Sample 100 images per class\n",
    "    \n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img_resized, img_gray = preprocess_image(img_path)\n",
    "        \n",
    "        all_images.append(img_resized)\n",
    "        all_labels.append(cls)\n",
    "        \n",
    "        color_hist_features.append(extract_color_histogram(img_resized))\n",
    "        hog_features.append(extract_hog_features(img_gray))\n",
    "        lbp_features.append(extract_lbp_features(img_gray))\n",
    "\n",
    "# Combine features\n",
    "X = np.hstack((hog_features, color_hist_features, lbp_features))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(all_labels)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LDA(n_components=None)\n",
    "X_lda = lda.fit_transform(X_scaled, Y)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lda, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# ------------------------- SVM -------------------------\n",
    "svm_params = {\n",
    "    'C': [0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_model = SVC()\n",
    "grid_svm = GridSearchCV(svm_model, svm_params, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_svm.fit(X_train, Y_train)\n",
    "best_svm_model = grid_svm.best_estimator_\n",
    "\n",
    "# ------------------------- Decision Tree -------------------------\n",
    "dt_params = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "grid_dt = GridSearchCV(dt_model, dt_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_dt.fit(X_train, Y_train)\n",
    "best_dt = grid_dt.best_estimator_\n",
    "\n",
    "# ------------------------- Random Forest -------------------------\n",
    "rf_params = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_rf = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_rf.fit(X_train, Y_train)\n",
    "best_rf_model = grid_rf.best_estimator_\n",
    "\n",
    "# ------------------------- Ensemble -------------------------\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', best_svm_model),\n",
    "        ('dt', best_dt),\n",
    "        ('rf', best_rf_model)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "ensemble.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluation\n",
    "Y_pred = ensemble.predict(X_test)\n",
    "print(f\"Ensemble Accuracy: {accuracy_score(Y_test, Y_pred) * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save all required components\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(lda, 'lda.pkl')\n",
    "joblib.dump(ensemble, 'ensemble_model.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "print(\"âœ… All models and components saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70aa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5234914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.6.1\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from scikit-learn==1.6.1) (1.15.1)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.1\n",
      "    Uninstalling joblib-1.1.1:\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.1\n",
      "    Uninstalling scikit-learn-1.2.1:\n",
      "      Successfully uninstalled scikit-learn-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\krish\\\\anaconda3\\\\Lib\\\\site-packages\\\\~klearn\\\\utils\\\\murmurhash.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\krish\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.6.1 --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53572c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
